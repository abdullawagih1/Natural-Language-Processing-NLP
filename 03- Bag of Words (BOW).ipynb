{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.11"},"papermill":{"default_parameters":{},"duration":28.842881,"end_time":"2023-02-20T20:26:40.760365","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2023-02-20T20:26:11.917484","version":"2.3.4"}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Bag-of-Words\n\n## <p style=\"font-family:JetBrains Mono; font-weight:normal; letter-spacing: 1px; color:#207d06; font-size:100%; text-align:left;padding: 0px; border-bottom: 3px solid #207d06;\">The idea</p>\n\nAfter tokenization and pre-processing, we are left with **variable length** sequences of text, but the problem is machine learning algorithms require **fixed length** vectors of numbers. \n\nThe **simplest** approach to overcome this is by using a **bag-of-words**, which simply **counts how many times each word appears** in a document. It's called a **bag** because the **order of the words is ignored** - we only care about whether a word appeared or not. \n\n<center>\n<img src='https://i.postimg.cc/pLvhy7zs/basicbow.png' width=600>\n</center>\n<br>\n\nThe linguistic reasoning behind this approach is that **similar documents share similar vocabularies**. For example, football articles will often use words like *score*, *pass*, *team* whereas weather reports will use a completely different set of words like *rain*, *sun*, *umbrella*.\n\nWe might want to **remove stop words** (common words that have little meaning like *the*, *of*, *how*) to make it easier to identify similar documents as these will be in pretty much all documents. ","metadata":{"papermill":{"duration":0.009552,"end_time":"2023-02-20T20:26:22.836571","exception":false,"start_time":"2023-02-20T20:26:22.827019","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"## <p style=\"font-family:JetBrains Mono; font-weight:normal; letter-spacing: 1px; color:#207d06; font-size:100%; text-align:left;padding: 0px; border-bottom: 3px solid #207d06;\">Binary Bag-of-Words</p>\n\nFor now, we'll focus on the **binary** version of a bag-of-words. This just indicates **whether a word appeared or not**, ignoring word order and word frequency.\n\n<center>\n<img src='https://i.postimg.cc/LsBRs9js/binarybow.jpg' width=600>\n</center>\n\nEach **row** in a **binary bag-of-words matrix** corresponds to a **single document** in the corpus. Each **column** corresponds to a **token** in the vocabulary. Note that the order of the tokens isn't important but it does need to be **fixed beforehand** when building the vocabulary.  \n\nTo **construct** the matrix, we place a 1 in entry (i,j) if and only if the j-th token appears in the i-th document and a 0 otherwise. \n\nFor a **general** bag-of-words, the (i,j) entry would instead be the **frequency** of the j-th token in the i-th document (but we will see there are better ways to encode frequency later).","metadata":{"papermill":{"duration":0.00896,"end_time":"2023-02-20T20:26:22.854538","exception":false,"start_time":"2023-02-20T20:26:22.845578","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# Similarity\n\n## <p style=\"font-family:JetBrains Mono; font-weight:normal; letter-spacing: 1px; color:#207d06; font-size:100%; text-align:left;padding: 0px; border-bottom: 3px solid #207d06;\">Vector Space Model</p>\n\nWe have gone from thinking of documents as a sequence of words to **points in a multi-dimensional vector space**. Importantly, the dimension of this space if **fixed**, i.e. each vector has the same length.\n\n<center>\n<img src='https://i.postimg.cc/L5nfB0Vr/unitcube.png' width=400>\n</center>\n<br>\n\nThis is very useful because it now allows us to **measure the distances** between these points among other things. Points (documents) that are close together will correspond to documents being **similar** in their vocabularies. ","metadata":{"papermill":{"duration":0.008651,"end_time":"2023-02-20T20:26:22.872352","exception":false,"start_time":"2023-02-20T20:26:22.863701","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"## <p style=\"font-family:JetBrains Mono; font-weight:normal; letter-spacing: 1px; color:#207d06; font-size:100%; text-align:left;padding: 0px; border-bottom: 3px solid #207d06;\">Cosine Similarity</p>\n\nThere are many **metrics** we could use to measure how 'close' two points are. For example, we could consider using the Euclidean distance, Manhattan distance or even Hamming distance. However, if documents in the same corpus have very different lengths, or the vocabulary is extremely large, these metrics become less reliable. \n\n$$\n\\Large\n\\cos(\\theta) = \\frac{a \\cdot b}{\\|a\\| \\|b\\|}\n$$\n\n<br>\n\nInstead, in the NLP domain it is much more common to use **Cosine Similarity**. This measures the **cosine of the angle** between any two points (more precisely their vectors starting from the origin). The **closer the score 1**, the smaller the angle between the vectors and the **more similar** the documents are. \n\n<br>\n<center>\n<img src='https://i.postimg.cc/DzSnKShk/cosine-similarity-vectors-original.jpg' width=800>\n</center>\n<br>\n\nNote that the **threshold** used to decide whether two documents are similar will **change depending on the application** and it can be anywhere between 0 and 1. It will be sensitive to how we pre-process our text. Lemmatization and stop word removal can help reduce the size of the vocabulary making it easier to identify similar documents.","metadata":{"papermill":{"duration":0.008661,"end_time":"2023-02-20T20:26:22.891809","exception":false,"start_time":"2023-02-20T20:26:22.883148","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# Encoding context\n\n## <p style=\"font-family:JetBrains Mono; font-weight:normal; letter-spacing: 1px; color:#207d06; font-size:100%; text-align:left;padding: 0px; border-bottom: 3px solid #207d06;\">Drawbacks to Bag-of-Words</p>\n\nWhilst using a bag-of-words is a great tool for **simple** NLP applications, it does have a number of drawbacks that we need to be aware about.\n\n* There is no way to handle **Out-of-Vocabulary** (OOV) words. If a new word appears in a later document, it will just be dropped.\n* It creates **sparse matrices** which can be inefficient, although we can overcome this by using a dictionary representation. \n* It isn't able to capture similarity between **synonyms**. \n* Word order is lost so words have **no relationship** to each other. For example, \"man eats bread\" is very different to \"bread eats man\" but they would have the same representations.\n","metadata":{"papermill":{"duration":0.008747,"end_time":"2023-02-20T20:26:22.909611","exception":false,"start_time":"2023-02-20T20:26:22.900864","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"## <p style=\"font-family:JetBrains Mono; font-weight:normal; letter-spacing: 1px; color:#207d06; font-size:100%; text-align:left;padding: 0px; border-bottom: 3px solid #207d06;\">n-grams</p>\n\nOne way to get around the problem of losing word order information is to use **n-grams**. This is when we group **chunks of n tokens** together to behave as if they were a single token. \n\nA 2-gram (aka **bigram**) would have 2 tokens per chunk, a 3-gram (aka **trigram**) would have 3 tokens per chuck, etc. \n\n<center>\n<img src='https://i.postimg.cc/ZncZPgp4/8ARA1.png' width=600>\n</center>\n<br>\n\nThis helps us capture **some context** that using single tokens wouldn't. The **vocabulary** then becomes the **collection of n-grams** produced. Depending on the application, you might want to use unigrams and bigrams together or just bigrams. You could even filter out bigrams that aren't useful for your application (e.g. only keep highly frequent or noun-noun bigrams).\n\nMeasuring **similarity** is exactly the **same as before**. However, using n-grams can **significantly increase the size of the vocabulary** making computations slower. There is therefore a tradeoff between contextual information added and increased computational time for modelling. \n\n","metadata":{"papermill":{"duration":0.008656,"end_time":"2023-02-20T20:26:22.927304","exception":false,"start_time":"2023-02-20T20:26:22.918648","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# Application\n\n## <p style=\"font-family:JetBrains Mono; font-weight:normal; letter-spacing: 1px; color:#207d06; font-size:100%; text-align:left;padding: 0px; border-bottom: 3px solid #207d06;\">Bag-of-Words using sklearn</p>","metadata":{"papermill":{"duration":0.00869,"end_time":"2023-02-20T20:26:22.945178","exception":false,"start_time":"2023-02-20T20:26:22.936488","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"Import the **libraries**.","metadata":{"papermill":{"duration":0.008643,"end_time":"2023-02-20T20:26:22.962846","exception":false,"start_time":"2023-02-20T20:26:22.954203","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import spacy\nimport numpy as np\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity","metadata":{"execution":{"iopub.execute_input":"2023-02-20T20:26:22.983236Z","iopub.status.busy":"2023-02-20T20:26:22.982428Z","iopub.status.idle":"2023-02-20T20:26:35.799727Z","shell.execute_reply":"2023-02-20T20:26:35.798343Z"},"papermill":{"duration":12.83109,"end_time":"2023-02-20T20:26:35.802954","exception":false,"start_time":"2023-02-20T20:26:22.971864","status":"completed"},"tags":[]},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"Define the **corpus**. Here we use some of the top news stories from 2022.","metadata":{"papermill":{"duration":0.008675,"end_time":"2023-02-20T20:26:35.820878","exception":false,"start_time":"2023-02-20T20:26:35.812203","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# A corpus containing a collection of sentences\ncorpus = [\n    \"Inflation surges around the world.\",\n    \"The Omicron coronavirus variant spreads.\",\n    \"World population exceeds 8 billion.\",\n    \"AI predicts protein structures.\"\n]","metadata":{"execution":{"iopub.execute_input":"2023-02-20T20:26:35.840812Z","iopub.status.busy":"2023-02-20T20:26:35.840144Z","iopub.status.idle":"2023-02-20T20:26:35.845872Z","shell.execute_reply":"2023-02-20T20:26:35.844509Z"},"papermill":{"duration":0.018568,"end_time":"2023-02-20T20:26:35.848428","exception":false,"start_time":"2023-02-20T20:26:35.829860","status":"completed"},"tags":[]},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"We will use **sklearn's CountVectorizer** to create a bag-of-words matrix.","metadata":{"papermill":{"duration":0.008491,"end_time":"2023-02-20T20:26:35.866245","exception":false,"start_time":"2023-02-20T20:26:35.857754","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Initialize vectorizer\nvectorizer = CountVectorizer()","metadata":{"execution":{"iopub.execute_input":"2023-02-20T20:26:35.886244Z","iopub.status.busy":"2023-02-20T20:26:35.885823Z","iopub.status.idle":"2023-02-20T20:26:35.891048Z","shell.execute_reply":"2023-02-20T20:26:35.889655Z"},"papermill":{"duration":0.018309,"end_time":"2023-02-20T20:26:35.893527","exception":false,"start_time":"2023-02-20T20:26:35.875218","status":"completed"},"tags":[]},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"The `.fit_transform` method learns a **vocabulary** from the corpus and returns the **bag-of-words matrix**.","metadata":{"papermill":{"duration":0.009016,"end_time":"2023-02-20T20:26:35.911481","exception":false,"start_time":"2023-02-20T20:26:35.902465","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Fit vectorizer to corpus\nbow = vectorizer.fit_transform(corpus)","metadata":{"execution":{"iopub.execute_input":"2023-02-20T20:26:35.931879Z","iopub.status.busy":"2023-02-20T20:26:35.930755Z","iopub.status.idle":"2023-02-20T20:26:35.940526Z","shell.execute_reply":"2023-02-20T20:26:35.939390Z"},"papermill":{"duration":0.022631,"end_time":"2023-02-20T20:26:35.943147","exception":false,"start_time":"2023-02-20T20:26:35.920516","status":"completed"},"tags":[]},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"We can see the **vocabulary dictionary** mapping using the `.vocabulary_` method. We could also use the `.get_feature_names_out()` to see just the words.","metadata":{"papermill":{"duration":0.00858,"end_time":"2023-02-20T20:26:35.960725","exception":false,"start_time":"2023-02-20T20:26:35.952145","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# View vocabulary\nvectorizer.vocabulary_","metadata":{"execution":{"iopub.execute_input":"2023-02-20T20:26:35.980664Z","iopub.status.busy":"2023-02-20T20:26:35.979635Z","iopub.status.idle":"2023-02-20T20:26:35.990144Z","shell.execute_reply":"2023-02-20T20:26:35.988945Z"},"papermill":{"duration":0.022887,"end_time":"2023-02-20T20:26:35.992481","exception":false,"start_time":"2023-02-20T20:26:35.969594","status":"completed"},"tags":[]},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":["{'inflation': 5,\n"," 'surges': 12,\n"," 'around': 1,\n"," 'the': 13,\n"," 'world': 15,\n"," 'omicron': 6,\n"," 'coronavirus': 3,\n"," 'variant': 14,\n"," 'spreads': 10,\n"," 'population': 7,\n"," 'exceeds': 4,\n"," 'billion': 2,\n"," 'ai': 0,\n"," 'predicts': 8,\n"," 'protein': 9,\n"," 'structures': 11}"]},"metadata":{}}]},{"cell_type":"markdown","source":"The vectorizer **output** is a **compressed sparse row matrix**, which is done to **improve memory efficiency**. ","metadata":{"papermill":{"duration":0.008647,"end_time":"2023-02-20T20:26:36.010599","exception":false,"start_time":"2023-02-20T20:26:36.001952","status":"completed"},"tags":[]}},{"cell_type":"code","source":"bow","metadata":{"execution":{"iopub.execute_input":"2023-02-20T20:26:36.031969Z","iopub.status.busy":"2023-02-20T20:26:36.031524Z","iopub.status.idle":"2023-02-20T20:26:36.038445Z","shell.execute_reply":"2023-02-20T20:26:36.037332Z"},"papermill":{"duration":0.020547,"end_time":"2023-02-20T20:26:36.040891","exception":false,"start_time":"2023-02-20T20:26:36.020344","status":"completed"},"tags":[]},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":["<4x16 sparse matrix of type '<class 'numpy.int64'>'\n","\twith 18 stored elements in Compressed Sparse Row format>"]},"metadata":{}}]},{"cell_type":"code","source":"print(bow)","metadata":{"execution":{"iopub.execute_input":"2023-02-20T20:26:36.061208Z","iopub.status.busy":"2023-02-20T20:26:36.060685Z","iopub.status.idle":"2023-02-20T20:26:36.068220Z","shell.execute_reply":"2023-02-20T20:26:36.067275Z"},"papermill":{"duration":0.020542,"end_time":"2023-02-20T20:26:36.070763","exception":false,"start_time":"2023-02-20T20:26:36.050221","status":"completed"},"tags":[]},"execution_count":7,"outputs":[{"name":"stdout","output_type":"stream","text":"  (0, 5)\t1\n\n  (0, 12)\t1\n\n  (0, 1)\t1\n\n  (0, 13)\t1\n\n  (0, 15)\t1\n\n  (1, 13)\t1\n\n  (1, 6)\t1\n\n  (1, 3)\t1\n\n  (1, 14)\t1\n\n  (1, 10)\t1\n\n  (2, 15)\t1\n\n  (2, 7)\t1\n\n  (2, 4)\t1\n\n  (2, 2)\t1\n\n  (3, 0)\t1\n\n  (3, 8)\t1\n\n  (3, 9)\t1\n\n  (3, 11)\t1\n"}]},{"cell_type":"markdown","source":"To convert the sparse matrix into a **dense matrix**, we call the `.toarray()` method.","metadata":{"papermill":{"duration":0.009087,"end_time":"2023-02-20T20:26:36.089417","exception":false,"start_time":"2023-02-20T20:26:36.080330","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Dense matrix representation\nbow.toarray()","metadata":{"execution":{"iopub.execute_input":"2023-02-20T20:26:36.109856Z","iopub.status.busy":"2023-02-20T20:26:36.109404Z","iopub.status.idle":"2023-02-20T20:26:36.117204Z","shell.execute_reply":"2023-02-20T20:26:36.115995Z"},"papermill":{"duration":0.020823,"end_time":"2023-02-20T20:26:36.119671","exception":false,"start_time":"2023-02-20T20:26:36.098848","status":"completed"},"tags":[]},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":["array([[0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1],\n","       [0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0],\n","       [0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1],\n","       [1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0]], dtype=int64)"]},"metadata":{}}]},{"cell_type":"markdown","source":"Notice how sklearn lower-cased and **tokenized the corpus for us**. Next we will do the same using our own **custom tokenizer**, which will give us **more control** over how the text is pre-processed. ","metadata":{"papermill":{"duration":0.009037,"end_time":"2023-02-20T20:26:36.138627","exception":false,"start_time":"2023-02-20T20:26:36.129590","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"## <p style=\"font-family:JetBrains Mono; font-weight:normal; letter-spacing: 1px; color:#207d06; font-size:100%; text-align:left;padding: 0px; border-bottom: 3px solid #207d06;\">Custom tokenizer using spacy</p>\n\nTo do this, we need to define our custom tokenizer as a **function** that given a document, **returns a list of tokens**. ","metadata":{"papermill":{"duration":0.00903,"end_time":"2023-02-20T20:26:36.157123","exception":false,"start_time":"2023-02-20T20:26:36.148093","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Load english language model\nnlp = spacy.load('en_core_web_sm')\n\n# Define custom tokenizer (remove stop words and punctuation and apply lemmatization)\ndef custom_tokenizer(doc):\n    return [t.lemma_ for t in nlp(doc) if (not t.is_punct) and (not t.is_stop)]","metadata":{"execution":{"iopub.execute_input":"2023-02-20T20:26:36.177808Z","iopub.status.busy":"2023-02-20T20:26:36.177395Z","iopub.status.idle":"2023-02-20T20:26:37.046385Z","shell.execute_reply":"2023-02-20T20:26:37.045101Z"},"papermill":{"duration":0.882901,"end_time":"2023-02-20T20:26:37.049558","exception":false,"start_time":"2023-02-20T20:26:36.166657","status":"completed"},"tags":[]},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"The tokenizer is then passed as a **callback function** inside the count vectorizer. We also set binary equal to true to produce a **binary** bag-of-words.","metadata":{"papermill":{"duration":0.009094,"end_time":"2023-02-20T20:26:37.068537","exception":false,"start_time":"2023-02-20T20:26:37.059443","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Pass tokenizer as callback function to countvectorizer\nvectorizer = CountVectorizer(tokenizer=custom_tokenizer, binary=True)\n\n# Fit vectorizer to corpus\nbow = vectorizer.fit_transform(corpus)","metadata":{"execution":{"iopub.execute_input":"2023-02-20T20:26:37.089399Z","iopub.status.busy":"2023-02-20T20:26:37.088945Z","iopub.status.idle":"2023-02-20T20:26:37.143032Z","shell.execute_reply":"2023-02-20T20:26:37.141496Z"},"papermill":{"duration":0.068061,"end_time":"2023-02-20T20:26:37.146147","exception":false,"start_time":"2023-02-20T20:26:37.078086","status":"completed"},"tags":[]},"execution_count":10,"outputs":[{"name":"stderr","output_type":"stream","text":"c:\\Users\\DESKTOP\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n\n  warnings.warn(\n"}]},{"cell_type":"markdown","source":"We can view the resulting **vocabulary** and matrix the same way as before. ","metadata":{"papermill":{"duration":0.009161,"end_time":"2023-02-20T20:26:37.164922","exception":false,"start_time":"2023-02-20T20:26:37.155761","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Vocabulary\nvectorizer.vocabulary_","metadata":{"execution":{"iopub.execute_input":"2023-02-20T20:26:37.185752Z","iopub.status.busy":"2023-02-20T20:26:37.185295Z","iopub.status.idle":"2023-02-20T20:26:37.193573Z","shell.execute_reply":"2023-02-20T20:26:37.192393Z"},"papermill":{"duration":0.021798,"end_time":"2023-02-20T20:26:37.196260","exception":false,"start_time":"2023-02-20T20:26:37.174462","status":"completed"},"tags":[]},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":["{'inflation': 5,\n"," 'surge': 12,\n"," 'world': 14,\n"," 'omicron': 6,\n"," 'coronavirus': 3,\n"," 'variant': 13,\n"," 'spread': 10,\n"," 'population': 7,\n"," 'exceed': 4,\n"," '8': 0,\n"," 'billion': 2,\n"," 'ai': 1,\n"," 'predict': 8,\n"," 'protein': 9,\n"," 'structure': 11}"]},"metadata":{}}]},{"cell_type":"code","source":"# Dense matrix representation\nbow.toarray()","metadata":{"execution":{"iopub.execute_input":"2023-02-20T20:26:37.217653Z","iopub.status.busy":"2023-02-20T20:26:37.217245Z","iopub.status.idle":"2023-02-20T20:26:37.225666Z","shell.execute_reply":"2023-02-20T20:26:37.224301Z"},"papermill":{"duration":0.02231,"end_time":"2023-02-20T20:26:37.228355","exception":false,"start_time":"2023-02-20T20:26:37.206045","status":"completed"},"tags":[]},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":["array([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1],\n","       [0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0],\n","       [1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1],\n","       [0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0]], dtype=int64)"]},"metadata":{}}]},{"cell_type":"markdown","source":"The sparse matrix can be **sliced and indexed** like a normal array.","metadata":{"papermill":{"duration":0.009533,"end_time":"2023-02-20T20:26:37.248180","exception":false,"start_time":"2023-02-20T20:26:37.238647","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Sparse slice\nprint(bow[:,0:4])","metadata":{"execution":{"iopub.execute_input":"2023-02-20T20:26:37.269425Z","iopub.status.busy":"2023-02-20T20:26:37.268998Z","iopub.status.idle":"2023-02-20T20:26:37.277379Z","shell.execute_reply":"2023-02-20T20:26:37.276052Z"},"papermill":{"duration":0.021918,"end_time":"2023-02-20T20:26:37.279871","exception":false,"start_time":"2023-02-20T20:26:37.257953","status":"completed"},"tags":[]},"execution_count":13,"outputs":[{"name":"stdout","output_type":"stream","text":"  (1, 3)\t1\n\n  (2, 0)\t1\n\n  (2, 2)\t1\n\n  (3, 1)\t1\n"}]},{"cell_type":"markdown","source":"## <p style=\"font-family:JetBrains Mono; font-weight:normal; letter-spacing: 1px; color:#207d06; font-size:100%; text-align:left;padding: 0px; border-bottom: 3px solid #207d06;\">Document Similarity</p>\n\nHere we will measure the **cosine similarity** between the documents in our corpus.","metadata":{"papermill":{"duration":0.009506,"end_time":"2023-02-20T20:26:37.299823","exception":false,"start_time":"2023-02-20T20:26:37.290317","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Cosine similarity using numpy\ndef cosine_sim(a,b):\n    return np.dot(a,b)/(np.linalg.norm(a)*np.linalg.norm(b))","metadata":{"execution":{"iopub.execute_input":"2023-02-20T20:26:37.323363Z","iopub.status.busy":"2023-02-20T20:26:37.322930Z","iopub.status.idle":"2023-02-20T20:26:37.329317Z","shell.execute_reply":"2023-02-20T20:26:37.327933Z"},"papermill":{"duration":0.021032,"end_time":"2023-02-20T20:26:37.331991","exception":false,"start_time":"2023-02-20T20:26:37.310959","status":"completed"},"tags":[]},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# Similarity between two documents\nprint(corpus[1])\nprint(corpus[3])\nprint(f'Similarity score: {cosine_sim(bow[1].toarray().squeeze(),bow[3].toarray().squeeze()):.3f}')","metadata":{"execution":{"iopub.execute_input":"2023-02-20T20:26:37.354543Z","iopub.status.busy":"2023-02-20T20:26:37.354134Z","iopub.status.idle":"2023-02-20T20:26:37.361942Z","shell.execute_reply":"2023-02-20T20:26:37.360577Z"},"papermill":{"duration":0.021783,"end_time":"2023-02-20T20:26:37.364614","exception":false,"start_time":"2023-02-20T20:26:37.342831","status":"completed"},"tags":[]},"execution_count":15,"outputs":[{"name":"stdout","output_type":"stream","text":"The Omicron coronavirus variant spreads.\n\nAI predicts protein structures.\n\nSimilarity score: 0.000\n"}]},{"cell_type":"code","source":"# Similarity between two documents\nprint(corpus[0])\nprint(corpus[2])\nprint(f'Similarity score: {cosine_sim(bow[0].toarray().squeeze(),bow[2].toarray().squeeze()):.3f}')","metadata":{"execution":{"iopub.execute_input":"2023-02-20T20:26:37.386779Z","iopub.status.busy":"2023-02-20T20:26:37.386367Z","iopub.status.idle":"2023-02-20T20:26:37.394173Z","shell.execute_reply":"2023-02-20T20:26:37.392634Z"},"papermill":{"duration":0.022003,"end_time":"2023-02-20T20:26:37.397017","exception":false,"start_time":"2023-02-20T20:26:37.375014","status":"completed"},"tags":[]},"execution_count":16,"outputs":[{"name":"stdout","output_type":"stream","text":"Inflation surges around the world.\n\nWorld population exceeds 8 billion.\n\nSimilarity score: 0.258\n"}]},{"cell_type":"markdown","source":"We can also use sklearn's `cosine_similarity`. This calculates all the **pairwise similarities** and returns the result in a matrix indexed by the documents.","metadata":{"papermill":{"duration":0.009663,"end_time":"2023-02-20T20:26:37.416844","exception":false,"start_time":"2023-02-20T20:26:37.407181","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# cosine_similarity takes either array-likes or sparse matrices\nprint(cosine_similarity(bow))","metadata":{"execution":{"iopub.execute_input":"2023-02-20T20:26:37.439305Z","iopub.status.busy":"2023-02-20T20:26:37.438897Z","iopub.status.idle":"2023-02-20T20:26:37.449551Z","shell.execute_reply":"2023-02-20T20:26:37.448323Z"},"papermill":{"duration":0.025535,"end_time":"2023-02-20T20:26:37.452364","exception":false,"start_time":"2023-02-20T20:26:37.426829","status":"completed"},"tags":[]},"execution_count":17,"outputs":[{"name":"stdout","output_type":"stream","text":"[[1.         0.         0.25819889 0.        ]\n\n [0.         1.         0.         0.        ]\n\n [0.25819889 0.         1.         0.        ]\n\n [0.         0.         0.         1.        ]]\n"}]},{"cell_type":"markdown","source":"## <p style=\"font-family:JetBrains Mono; font-weight:normal; letter-spacing: 1px; color:#207d06; font-size:100%; text-align:left;padding: 0px; border-bottom: 3px solid #207d06;\">n-grams</p>\n\nFinally, we will build a bag-of-words matrix using **n-grams**. To do this, we can pass the `ngram_range` parameter in countvectorizer. It takes in a tuple, with the **first entry** indicating the **minimum** chunk size and the **second entry** indicating the **maximum** chunk size.","metadata":{"papermill":{"duration":0.009759,"end_time":"2023-02-20T20:26:37.472221","exception":false,"start_time":"2023-02-20T20:26:37.462462","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Unigrams and bigrams with ngram_range=(1,2)\nvectorizer = CountVectorizer(tokenizer=custom_tokenizer, lowercase=False, binary=True, ngram_range=(1,2))\n\n# Fit vectorizer to corpus\nunibigrams = vectorizer.fit_transform(corpus)\n\n# Print vocabulary size\nprint(f'Size of vocabulary: {len(vectorizer.get_feature_names_out())}')\n\n# Print vocabulary\nprint(vectorizer.vocabulary_)","metadata":{"execution":{"iopub.execute_input":"2023-02-20T20:26:37.494203Z","iopub.status.busy":"2023-02-20T20:26:37.493768Z","iopub.status.idle":"2023-02-20T20:26:37.531349Z","shell.execute_reply":"2023-02-20T20:26:37.530248Z"},"papermill":{"duration":0.051787,"end_time":"2023-02-20T20:26:37.534216","exception":false,"start_time":"2023-02-20T20:26:37.482429","status":"completed"},"tags":[]},"execution_count":18,"outputs":[{"name":"stdout","output_type":"stream","text":"Size of vocabulary: 27\n\n{'inflation': 11, 'surge': 21, 'world': 25, 'inflation surge': 12, 'surge world': 22, 'Omicron': 4, 'coronavirus': 7, 'variant': 23, 'spread': 19, 'Omicron coronavirus': 5, 'coronavirus variant': 8, 'variant spread': 24, 'population': 13, 'exceed': 9, '8': 0, 'billion': 6, 'world population': 26, 'population exceed': 14, 'exceed 8': 10, '8 billion': 1, 'AI': 2, 'predict': 15, 'protein': 17, 'structure': 20, 'AI predict': 3, 'predict protein': 16, 'protein structure': 18}\n"}]},{"cell_type":"code","source":"# Only bigrams with ngram_range=(2,2)\nvectorizer = CountVectorizer(tokenizer=custom_tokenizer, lowercase=False, binary=True, ngram_range=(2,2))\n\n# Fit vectorizer to corpus\nbigrams = vectorizer.fit_transform(corpus)\n\n# Print vocabulary size\nprint(f'Size of vocabulary: {len(vectorizer.get_feature_names_out())}')\n\n# Print vocabulary\nprint(vectorizer.vocabulary_)","metadata":{"execution":{"iopub.execute_input":"2023-02-20T20:26:37.557046Z","iopub.status.busy":"2023-02-20T20:26:37.556623Z","iopub.status.idle":"2023-02-20T20:26:37.593702Z","shell.execute_reply":"2023-02-20T20:26:37.592525Z"},"papermill":{"duration":0.052004,"end_time":"2023-02-20T20:26:37.596791","exception":false,"start_time":"2023-02-20T20:26:37.544787","status":"completed"},"tags":[]},"execution_count":19,"outputs":[{"name":"stdout","output_type":"stream","text":"Size of vocabulary: 12\n\n{'inflation surge': 5, 'surge world': 9, 'Omicron coronavirus': 2, 'coronavirus variant': 3, 'variant spread': 10, 'world population': 11, 'population exceed': 6, 'exceed 8': 4, '8 billion': 0, 'AI predict': 1, 'predict protein': 7, 'protein structure': 8}\n"}]}]}