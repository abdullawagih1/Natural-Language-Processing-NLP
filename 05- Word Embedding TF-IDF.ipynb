{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.11"},"papermill":{"default_parameters":{},"duration":54.86958,"end_time":"2023-02-20T20:31:40.113540","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2023-02-20T20:30:45.243960","version":"2.3.4"}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Word Embedding\n\n## <p style=\"font-family:JetBrains Mono; font-weight:normal; letter-spacing: 1px; color:#207d06; font-size:100%; text-align:left;padding: 0px; border-bottom: 3px solid #207d06;\">Shortcomings of Bag-of-Words</p>\n\n<center>\n<img src='https://i.postimg.cc/1zw5pqP1/wordcloud.jpg' width=500>\n</center>\n<br>\n\nLast time we discussed that with a bag-of-words we **lose word order information**, although this can be partially remedied by using n-grams to encode context. \n\nWhen working with a **binary** bag-of-words there is another significant drawback. This is that **all words are treated as equally important**, although we know this is **not the case** in language. \n\nWe could use a **frequency** bag-of-words but then some words like 'the' and 'it' **occur very frequently** and affect similarity calculations. We could remove stop words but this won't remove all of the frequent and redundant words. For example, in a corpus of food recipes, words like 'mix', 'bowl' and 'teaspoon' will appear in almost all documents and so won't be very informative.","metadata":{"papermill":{"duration":0.005962,"end_time":"2023-02-20T20:30:55.896223","exception":false,"start_time":"2023-02-20T20:30:55.890261","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"## <p style=\"font-family:JetBrains Mono; font-weight:normal; letter-spacing: 1px; color:#207d06; font-size:100%; text-align:left;padding: 0px; border-bottom: 3px solid #207d06;\">Relative frequency</p>\n\nAn improvement then would be to use the **relative frequency** of each word in the corpus. This is **calculated** as the number of times a word appears in a document divided by the total number of times it appears in the corpus. \n\n<br>\n\n$$\n\\large\n\\text{relative frequency} = \\frac{\\text{frequency in document}}{\\text{frequency in corpus}}\n$$\n\n<br>\n\nThe idea is that words that appear **highly frequently** in some documents and rarely in the rest, are likely to be **meaningful to those documents** and will help distinguish between documents. On the other hand, words that appears roughly **uniformly** across all documents are **unlikely to be important**. ","metadata":{"papermill":{"duration":0.00551,"end_time":"2023-02-20T20:30:55.907812","exception":false,"start_time":"2023-02-20T20:30:55.902302","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# TF-IDF\n\n## <p style=\"font-family:JetBrains Mono; font-weight:normal; letter-spacing: 1px; color:#207d06; font-size:100%; text-align:left;padding: 0px; border-bottom: 3px solid #207d06;\">Term Frequency</p>\n\n<center>\n<img src='https://i.postimg.cc/xCL5Dt2p/wren.jpg' width=600>\n</center>\n<br>\n\n**TF-IDF** stands for **Term Frequency - Inverse Document Frequency** and is made up of **two components**. The first is the **term frequency**. \n\nWhilst some people define the term frequency to be the relative frequency, it is more common to use the **raw frequency** of the token/term $t$ in document $d$.\n\n<br>\n\n$$\n\\Large\n\\text{tf}(t,d) = f_{t,d}\n$$\n\n<br>\n\nHowever, some documents may be much **longer** than others and so will naturally have higher frequencies across the board. For this reason, it is standard practice to apply the **log-transform** to reduce this bias. The term frequency then becomes\n\n<br>\n\n$$\n\\Large\n\\text{tf}(t,d) = \\log(1+f_{t,d})\n$$\n\n## <p style=\"font-family:JetBrains Mono; font-weight:normal; letter-spacing: 1px; color:#207d06; font-size:100%; text-align:left;padding: 0px; border-bottom: 3px solid #207d06;\">Inverse Document Frequency</p>\n\nThe second part of TF-IDF is the **inverse document frequency**. This is the part that will **emphasise the more important words** in each document. \n\nGiven a token/term $t$ in a corpus $D$, we define\n\n<br>\n\n$$\n\\Large\n\\text{idf}(t,D) = \\log \\left(\\frac{N}{n_t} \\right)\n$$\n\n<br>\n\nwhere $N$ is the number of documents and $n_t$ is the number of documents $t$ appears in. Notice how as $n_t$ decreases the idf increases corresponding to a token that is more likely to be **important**.\n\n## <p style=\"font-family:JetBrains Mono; font-weight:normal; letter-spacing: 1px; color:#207d06; font-size:100%; text-align:left;padding: 0px; border-bottom: 3px solid #207d06;\">TF-IDF score</p>\n\nTo get the final tf-idf score, we simply **multiply** the tf with the idf. That is,\n\n<br>\n\n$$\n\\Large\nw_{t,d} = \\text{tf}(t,d) \\times \\text{idf}(t,D)\n$$\n\n<br>\n\nSo the more frequently a word appears in a given document and the fewer times it appears in other documents the **higher** its TF-IDF score. \n\n## <p style=\"font-family:JetBrains Mono; font-weight:normal; letter-spacing: 1px; color:#207d06; font-size:100%; text-align:left;padding: 0px; border-bottom: 3px solid #207d06;\">Variations</p>\n\nThere are **many variations** to the TF-IDF score. Like we discussed earlier, instead of raw frequency, we could use the **relative frequency** in the term frequency term. This is actually how Wikipedia presents the formula.\n\nThe **sklearn implementation** of tf-idf doesn't apply the log-transform in the tf term. It also adds constants to the idf term to prevent division by zero and uses the **natural logarithm**. In particular, is uses the following formulas.\n\n<br>\n\n$$\n\\Large\n\\text{tf}(t,d) = f_{t,d}\n$$\n\n<br>\n\n$$\n\\Large\n\\text{idf}(t,D) = \\ln \\left(\\frac{N+1}{n_t+1} \\right) + 1\n$$\n\n<br>\n\nIt also **normalizes** the output vector for each document so that each document has a vector of scores with **norm equal to 1**. ","metadata":{"papermill":{"duration":0.005536,"end_time":"2023-02-20T20:30:55.919134","exception":false,"start_time":"2023-02-20T20:30:55.913598","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# Application\n\n## <p style=\"font-family:JetBrains Mono; font-weight:normal; letter-spacing: 1px; color:#207d06; font-size:100%; text-align:left;padding: 0px; border-bottom: 3px solid #207d06;\">TF-IDF with sklearn</p>\n\n<br>\n\nImport the **libraries**.","metadata":{"papermill":{"duration":0.005552,"end_time":"2023-02-20T20:30:55.930446","exception":false,"start_time":"2023-02-20T20:30:55.924894","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import spacy\nimport numpy as np\n\nfrom sklearn.datasets import fetch_20newsgroups\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity","metadata":{"execution":{"iopub.execute_input":"2023-02-20T20:30:55.944494Z","iopub.status.busy":"2023-02-20T20:30:55.943667Z","iopub.status.idle":"2023-02-20T20:31:09.678555Z","shell.execute_reply":"2023-02-20T20:31:09.677254Z"},"papermill":{"duration":13.745557,"end_time":"2023-02-20T20:31:09.681842","exception":false,"start_time":"2023-02-20T20:30:55.936285","status":"completed"},"tags":[]},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"For this demo, we'll use use the **20 newsgroups dataset**, which is a collection of 18,000 newsgroup posts across 20 topics. We'll take the posts relating to the `sci.space` topic as that will be enough for our application.","metadata":{"papermill":{"duration":0.005531,"end_time":"2023-02-20T20:31:09.693429","exception":false,"start_time":"2023-02-20T20:31:09.687898","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Load corpus\ncorpus = fetch_20newsgroups(categories=['sci.space'], remove=('headers', 'footers', 'quotes'))\n\n# Preview data\nprint(len(corpus.data))\nprint(corpus.data[0])","metadata":{"execution":{"iopub.execute_input":"2023-02-20T20:31:09.707432Z","iopub.status.busy":"2023-02-20T20:31:09.706355Z","iopub.status.idle":"2023-02-20T20:31:20.948784Z","shell.execute_reply":"2023-02-20T20:31:20.947179Z"},"papermill":{"duration":11.2536,"end_time":"2023-02-20T20:31:20.952784","exception":false,"start_time":"2023-02-20T20:31:09.699184","status":"completed"},"tags":[]},"execution_count":2,"outputs":[{"name":"stdout","output_type":"stream","text":"593\n\n\n\nAny lunar satellite needs fuel to do regular orbit corrections, and when\n\nits fuel runs out it will crash within months.  The orbits of the Apollo\n\nmotherships changed noticeably during lunar missions lasting only a few\n\ndays.  It is *possible* that there are stable orbits here and there --\n\nthe Moon's gravitational field is poorly mapped -- but we know of none.\n\n\n\nPerturbations from Sun and Earth are relatively minor issues at low\n\naltitudes.  The big problem is that the Moon's own gravitational field\n\nis quite lumpy due to the irregular distribution of mass within the Moon.\n"}]},{"cell_type":"markdown","source":"We need to **pre-process** the text first using a **tokenizer**. We'll do this using spacy, like we have seen in previous notebooks. We apply lemmatization, remove punctuation, spaces and non-alphabetic characters. ","metadata":{"papermill":{"duration":0.005564,"end_time":"2023-02-20T20:31:20.966389","exception":false,"start_time":"2023-02-20T20:31:20.960825","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Load english language model\nnlp = spacy.load('en_core_web_sm')\n\n# Disable named-entity recognition and parsing to save time\nunwanted_pipes = [\"ner\", \"parser\"]\n\n# Custom tokenizer using spacy\ndef custom_tokenizer(doc):\n    with nlp.disable_pipes(*unwanted_pipes):\n        return [t.lemma_ for t in nlp(doc) if not t.is_punct and not t.is_space and t.is_alpha]","metadata":{"execution":{"iopub.execute_input":"2023-02-20T20:31:20.979875Z","iopub.status.busy":"2023-02-20T20:31:20.979462Z","iopub.status.idle":"2023-02-20T20:31:21.847144Z","shell.execute_reply":"2023-02-20T20:31:21.845670Z"},"papermill":{"duration":0.87766,"end_time":"2023-02-20T20:31:21.849927","exception":false,"start_time":"2023-02-20T20:31:20.972267","status":"completed"},"tags":[]},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"Similar to the bag-of-words countvectorizer, **sklearn** also provides a class to perform **TF-IDF**, namely`TfidfVectorizer`. To use our custom tokenizer, we pass it through as a parameter.","metadata":{"papermill":{"duration":0.006253,"end_time":"2023-02-20T20:31:21.862417","exception":false,"start_time":"2023-02-20T20:31:21.856164","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Initialise tf-idf tokenizer\nvectorizer = TfidfVectorizer(tokenizer=custom_tokenizer)\n\n# Fit vectorizer to corpus\nfeatures = vectorizer.fit_transform(corpus.data)","metadata":{"execution":{"iopub.execute_input":"2023-02-20T20:31:21.877215Z","iopub.status.busy":"2023-02-20T20:31:21.876229Z","iopub.status.idle":"2023-02-20T20:31:36.512419Z","shell.execute_reply":"2023-02-20T20:31:36.511364Z"},"papermill":{"duration":14.646355,"end_time":"2023-02-20T20:31:36.515197","exception":false,"start_time":"2023-02-20T20:31:21.868842","status":"completed"},"tags":[]},"execution_count":4,"outputs":[{"name":"stderr","output_type":"stream","text":"c:\\Users\\DESKTOP\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n\n  warnings.warn(\n"}]},{"cell_type":"markdown","source":"The output is a **sparse matrix** with dimensions (number of documents, size of vocabulary). The entries of the matrix are the **tf-idf scores** of each **document-token pair**. ","metadata":{"papermill":{"duration":0.005667,"end_time":"2023-02-20T20:31:36.527125","exception":false,"start_time":"2023-02-20T20:31:36.521458","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Size of vocabulary\nprint(len(vectorizer.get_feature_names_out()))\n\n# Dimensions of output matrix\nprint(features.shape)","metadata":{"execution":{"iopub.execute_input":"2023-02-20T20:31:36.540929Z","iopub.status.busy":"2023-02-20T20:31:36.540526Z","iopub.status.idle":"2023-02-20T20:31:36.553270Z","shell.execute_reply":"2023-02-20T20:31:36.552189Z"},"papermill":{"duration":0.02271,"end_time":"2023-02-20T20:31:36.556008","exception":false,"start_time":"2023-02-20T20:31:36.533298","status":"completed"},"tags":[]},"execution_count":5,"outputs":[{"name":"stdout","output_type":"stream","text":"9440\n\n(593, 9440)\n"}]},{"cell_type":"code","source":"# What the matrix looks like\nprint(features)","metadata":{"execution":{"iopub.execute_input":"2023-02-20T20:31:36.570323Z","iopub.status.busy":"2023-02-20T20:31:36.569288Z","iopub.status.idle":"2023-02-20T20:31:36.576894Z","shell.execute_reply":"2023-02-20T20:31:36.575246Z"},"papermill":{"duration":0.017581,"end_time":"2023-02-20T20:31:36.579618","exception":false,"start_time":"2023-02-20T20:31:36.562037","status":"completed"},"tags":[]},"execution_count":6,"outputs":[{"name":"stdout","output_type":"stream","text":"  (0, 5064)\t0.10452754121963853\n\n  (0, 2351)\t0.12747025764625855\n\n  (0, 4340)\t0.15331700873692364\n\n  (0, 2459)\t0.10862435105627101\n\n  (0, 4916)\t0.17102715751031994\n\n  (0, 6702)\t0.09940033595823265\n\n  (0, 5982)\t0.10183554382071024\n\n  (0, 6514)\t0.08455482269873241\n\n  (0, 896)\t0.0892999596249832\n\n  (0, 316)\t0.1109487112663238\n\n  (0, 4896)\t0.08247641364333849\n\n  (0, 628)\t0.051044670776703174\n\n  (0, 4368)\t0.10270174012167517\n\n  (0, 5274)\t0.13259746290766442\n\n  (0, 6908)\t0.12524708704889775\n\n  (0, 2494)\t0.07376562213268434\n\n  (0, 8105)\t0.09513204666042695\n\n  (0, 3287)\t0.051874685324429695\n\n  (0, 6181)\t0.1390186329543497\n\n  (0, 5652)\t0.11219531673533985\n\n  (0, 4589)\t0.06321728493925476\n\n  (0, 9158)\t0.06158004812009137\n\n  (0, 1141)\t0.048918909156680825\n\n  (0, 5023)\t0.12320196834845284\n\n  (0, 6354)\t0.15331700873692364\n\n  :\t:\n\n  (592, 9313)\t0.0313001503814187\n\n  (592, 3999)\t0.06582915945746075\n\n  (592, 9397)\t0.20477163720551372\n\n  (592, 6129)\t0.031664982813140126\n\n  (592, 3187)\t0.07100762863378778\n\n  (592, 4076)\t0.04613594191502364\n\n  (592, 9286)\t0.04398758106578061\n\n  (592, 3446)\t0.02462764342748487\n\n  (592, 5829)\t0.03656235174780665\n\n  (592, 7389)\t0.029040380363968624\n\n  (592, 628)\t0.021742436441722357\n\n  (592, 4368)\t0.04374572356080208\n\n  (592, 3287)\t0.04419196094082858\n\n  (592, 4589)\t0.02692735164896627\n\n  (592, 5383)\t0.10200337834541046\n\n  (592, 812)\t0.06123880311404514\n\n  (592, 1)\t0.06820911364259086\n\n  (592, 5790)\t0.16890335679561622\n\n  (592, 8368)\t0.29796472745832864\n\n  (592, 9251)\t0.02550415981865799\n\n  (592, 4372)\t0.016302971093028248\n\n  (592, 371)\t0.10479693765659935\n\n  (592, 2372)\t0.05550092120951277\n\n  (592, 8491)\t0.19534386377244778\n\n  (592, 4918)\t0.11345213064337507\n"}]},{"cell_type":"markdown","source":"## <p style=\"font-family:JetBrains Mono; font-weight:normal; letter-spacing: 1px; color:#207d06; font-size:100%; text-align:left;padding: 0px; border-bottom: 3px solid #207d06;\">Document Search</p>\n\nNow we have the tf-idf matrix, we can **measure similarity** exactly the same as before - by using **cosine similarity**. Given a document, we can find the other documents which are **most similar to the original one**. \n\nWe will now go a step further and use it to build a basic **document search recommender system**. Given a **query** (i.e. a search term), we **transform** the query, **measure the similarity** with all the other documents and finally **return the most similar documents**. ","metadata":{"papermill":{"duration":0.005837,"end_time":"2023-02-20T20:31:36.591659","exception":false,"start_time":"2023-02-20T20:31:36.585822","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Transform the query\nquery = [\"Mars\"]\nquery_tfidf = vectorizer.transform(query)","metadata":{"execution":{"iopub.execute_input":"2023-02-20T20:31:36.605359Z","iopub.status.busy":"2023-02-20T20:31:36.604892Z","iopub.status.idle":"2023-02-20T20:31:36.615288Z","shell.execute_reply":"2023-02-20T20:31:36.614308Z"},"papermill":{"duration":0.020203,"end_time":"2023-02-20T20:31:36.617820","exception":false,"start_time":"2023-02-20T20:31:36.597617","status":"completed"},"tags":[]},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# Calculate pairwise similarity with all documents in corpus\ncosine_similarities = cosine_similarity(features, query_tfidf).flatten()","metadata":{"execution":{"iopub.execute_input":"2023-02-20T20:31:36.631712Z","iopub.status.busy":"2023-02-20T20:31:36.631316Z","iopub.status.idle":"2023-02-20T20:31:36.638288Z","shell.execute_reply":"2023-02-20T20:31:36.637293Z"},"papermill":{"duration":0.01664,"end_time":"2023-02-20T20:31:36.640575","exception":false,"start_time":"2023-02-20T20:31:36.623935","status":"completed"},"tags":[]},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# Return indices of top k matching documents\ndef top_k(arr, k):\n    kth_largest = (k + 1) * -1\n    return np.argsort(arr)[:kth_largest:-1]\n\n# Return top 5 document indices\ntop_related_indices = top_k(cosine_similarities, 5)\nprint(top_related_indices)","metadata":{"execution":{"iopub.execute_input":"2023-02-20T20:31:36.654963Z","iopub.status.busy":"2023-02-20T20:31:36.654031Z","iopub.status.idle":"2023-02-20T20:31:36.661719Z","shell.execute_reply":"2023-02-20T20:31:36.660238Z"},"papermill":{"duration":0.017529,"end_time":"2023-02-20T20:31:36.664256","exception":false,"start_time":"2023-02-20T20:31:36.646727","status":"completed"},"tags":[]},"execution_count":9,"outputs":[{"name":"stdout","output_type":"stream","text":"[468 583 410  79 343]\n"}]},{"cell_type":"code","source":"# Corresponding cosine similarities\nprint(cosine_similarities[top_related_indices])","metadata":{"execution":{"iopub.execute_input":"2023-02-20T20:31:36.678864Z","iopub.status.busy":"2023-02-20T20:31:36.678460Z","iopub.status.idle":"2023-02-20T20:31:36.684571Z","shell.execute_reply":"2023-02-20T20:31:36.683221Z"},"papermill":{"duration":0.015816,"end_time":"2023-02-20T20:31:36.686583","exception":false,"start_time":"2023-02-20T20:31:36.670767","status":"completed"},"tags":[]},"execution_count":10,"outputs":[{"name":"stdout","output_type":"stream","text":"[0.32658502 0.1810773  0.15383114 0.14742523 0.14398152]\n"}]},{"cell_type":"code","source":"# Top match\nprint(corpus.data[top_related_indices[0]])","metadata":{"execution":{"iopub.execute_input":"2023-02-20T20:31:36.700558Z","iopub.status.busy":"2023-02-20T20:31:36.700128Z","iopub.status.idle":"2023-02-20T20:31:36.705679Z","shell.execute_reply":"2023-02-20T20:31:36.704597Z"},"papermill":{"duration":0.015619,"end_time":"2023-02-20T20:31:36.708374","exception":false,"start_time":"2023-02-20T20:31:36.692755","status":"completed"},"tags":[]},"execution_count":11,"outputs":[{"name":"stdout","output_type":"stream","text":"What is the deal with life on Mars?  I save the \"face\" and heard \n\nassociated theories. (which sound thin to me)\n\n\n\nAre we going back to Mars to look at this face agian?\n\nDoes anyone buy all the life theories?\n\n\n"}]},{"cell_type":"code","source":"# Second-best match\nprint(corpus.data[top_related_indices[1]])","metadata":{"execution":{"iopub.execute_input":"2023-02-20T20:31:36.723733Z","iopub.status.busy":"2023-02-20T20:31:36.723315Z","iopub.status.idle":"2023-02-20T20:31:36.729769Z","shell.execute_reply":"2023-02-20T20:31:36.728345Z"},"papermill":{"duration":0.016933,"end_time":"2023-02-20T20:31:36.732227","exception":false,"start_time":"2023-02-20T20:31:36.715294","status":"completed"},"tags":[]},"execution_count":12,"outputs":[{"name":"stdout","output_type":"stream","text":"\n\nA practical suggestion, to be sure, but one could *also* peek into\n\nnews.lists, where Brian Reid has posted \"USENET Readership report for\n\nMar 93.\" Another posting called \"USENET READERSHIP SUMMARY REPORT FOR\n\nMAR 93\" gives the methodology and caveats of Reid's survey.  (These\n\npostings failed to appear for a while-- I wonder why?-- but they are\n\nnow back.)\n\n\n\nReid, alas, gives us no measure of the \"power/influence\" of readers...\n\nSorry, Mark.\n\n\n\nI suspect Mark, dangling out there on Fidonet, may not get news.lists\n\nso I've mailed him copies of these reports.\n\n\n\nThe bottom line?\n\n\n\n        +-- Estimated total number of people who read the group, worldwide.\n\n        |     +-- Actual number of readers in sampled population\n\n        |     |     +-- Propagation: how many sites receive this group at all\n\n        |     |     |      +-- Recent traffic (messages per month)\n\n        |     |     |      |      +-- Recent traffic (kilobytes per month)\n\n        |     |     |      |      |      +-- Crossposting percentage\n\n        |     |     |      |      |      |    +-- Cost ratio: $US/month/rdr\n\n        |     |     |      |      |      |    |      +-- Share: % of newsrders\n\n        |     |     |      |      |      |    |      |   who read this group.\n\n        V     V     V      V      V      V    V      V\n\n  88  62000  1493   80%  1958  4283.9    19%  0.10   2.9%  sci.space \n\n\n\nThe first figure indicates that sci.space ranks 88th among most-read\n\nnewsgroups.\n\n\n\nI've been keeping track sporadically to watch the growth of traffic\n\nand readership.  You might be entertained to see this.\n\n\n\nOct 91   55  71000  1387   84%   718  1865.2    21%  0.04   4.2%  sci.space\n\nMar 92   43  85000  1741   82%  1207  2727.2    13%  0.06   4.1%  sci.space\n\nJul 92   48  94000  1550   80%  1044  2448.3    12%  0.04   3.8%  sci.space\n\nMay 92   45  94000  2023   82%   834  1744.8    13%  0.04   4.1%  sci.space\n\n(some kind of glitch in estimating number of readers happens here)\n\nSep 92   45  51000  1690   80%  1420  3541.2    16%  0.11   3.6%  sci.space \n\nNov 92   78  47000  1372   81%  1220  2633.2    17%  0.08   2.8%  sci.space \n\n(revision in ranking groups happens here(?))\n\nMar 93   88  62000  1493   80%  1958  4283.9    19%  0.10   2.9%  sci.space \n\n\n\nPossibly old Usenet hands could give me some more background on how to\n\ninterpret these figures, glitches, or the history of Reid's reporting\n\neffort.  Take it to e-mail-- it doesn't belong in sci.space.\n"}]}]}